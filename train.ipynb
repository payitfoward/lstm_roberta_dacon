{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "43140f53",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import torch\n",
    "import os\n",
    "\n",
    "from transformers import AutoTokenizer, AutoConfig, TrainingArguments, Trainer\n",
    "from mkdataset import TypeDataset, EmotionDataset, TimeDataset, ConfidenceDataset, TestDataset\n",
    "from datasets import load_metric, load_dataset\n",
    "from classifier import RobertaForSequenceClassification\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9d7eb385",
   "metadata": {},
   "outputs": [],
   "source": [
    "seed = 777\n",
    "os.environ[\"PYTHONHASHSEED\"] = str(seed)\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)  # type: ignore\n",
    "torch.backends.cudnn.deterministic = True  # type: ignore\n",
    "torch.backends.cudnn.benchmark = True  # type: ignore"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "137027c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_metrics(pred):\n",
    "    f1 = load_metric(\"f1\")\n",
    "    references = pred.label_ids\n",
    "    predictions = pred.predictions.argmax(axis=1)\n",
    "    metric = f1.compute(predictions=predictions, references=references, average=\"micro\")\n",
    "    return metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "cd683a0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df = pd.read_csv(\"train.csv\")\n",
    "test_df = pd.read_csv(\"test.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "da72955f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n",
      "/opt/conda/lib/python3.7/site-packages/sklearn/utils/__init__.py:806: DeprecationWarning: `np.int` is a deprecated alias for the builtin `int`. To silence this warning, use `int` by itself. Doing this will not modify any behavior and is safe. When replacing `np.int`, you may wish to use e.g. `np.int64` or `np.int32` to specify the precision. If you wish to review your current use, check the release note link for additional information.\n",
      "Deprecated in NumPy 1.20; for more details and guidance: https://numpy.org/devdocs/release/1.20.0-notes.html#deprecations\n",
      "  return floored.astype(np.int)\n"
     ]
    }
   ],
   "source": [
    "train_type_df, valid_type_df = train_test_split(train_df, test_size=0.3, random_state=seed, stratify=train_df.유형)\n",
    "train_emotion_df, valid_emotion_df = train_test_split(train_df, test_size=0.3, random_state=seed, stratify=train_df.극성)\n",
    "train_time_df, valid_time_df = train_test_split(train_df, test_size=0.3, random_state=seed, stratify=train_df.시제)\n",
    "train_confidence_df, valid_confidence_df = train_test_split(train_df, test_size=0.3, random_state=seed, stratify=train_df.확실성)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "ad490eb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = AutoTokenizer.from_pretrained(\"klue/roberta-base\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "938d37c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_type_dataset = TypeDataset(data=train_type_df, tokenizer=tokenizer)\n",
    "train_emotion_dataset = EmotionDataset(data=train_emotion_df, tokenizer=tokenizer)\n",
    "train_time_dataset = TimeDataset(data=train_df, tokenizer=tokenizer)\n",
    "train_confidence_dataset = ConfidenceDataset(data=train_confidence_df, tokenizer=tokenizer)\n",
    "valid_type_dataset = TypeDataset(data=valid_type_df, tokenizer=tokenizer)\n",
    "valid_emotion_dataset = EmotionDataset(data=valid_emotion_df, tokenizer=tokenizer)\n",
    "valid_time_dataset = TimeDataset(data=valid_time_df, tokenizer=tokenizer)\n",
    "valid_confidence_dataset = ConfidenceDataset(data=valid_confidence_df, tokenizer=tokenizer)\n",
    "test_dataset = TestDataset(data=test_df, tokenizer=tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "4c3c7f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "gpu = torch.device(\"cuda:0\")\n",
    "cpu = torch.device(\"cpu\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "23a52bb5",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "type_config.num_labels = len(train_df.유형.value_counts())\n",
    "emotion_config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "emotion_config.num_labels = len(train_df.극성.value_counts())\n",
    "time_config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "time_config.num_labels = len(train_df.시제.value_counts())\n",
    "confidence_config = AutoConfig.from_pretrained(\"klue/roberta-base\")\n",
    "confidence_config.num_labels = len(train_df.확실성.value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "15480153",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l1_reverse', 'lstm.bias_ih_l1_reverse', 'lstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l1_reverse', 'lstm.bias_ih_l1_reverse', 'lstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l1_reverse', 'lstm.bias_ih_l1_reverse', 'lstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
      "Some weights of the model checkpoint at klue/roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.bias', 'lm_head.dense.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'lm_head.layer_norm.bias', 'lm_head.decoder.weight', 'lm_head.decoder.bias']\n",
      "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
      "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
      "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at klue/roberta-base and are newly initialized: ['roberta.pooler.dense.weight', 'roberta.pooler.dense.bias', 'lstm.weight_ih_l0', 'lstm.weight_hh_l0', 'lstm.bias_ih_l0', 'lstm.bias_hh_l0', 'lstm.weight_ih_l0_reverse', 'lstm.weight_hh_l0_reverse', 'lstm.bias_ih_l0_reverse', 'lstm.bias_hh_l0_reverse', 'lstm.weight_ih_l1', 'lstm.weight_hh_l1', 'lstm.bias_ih_l1', 'lstm.bias_hh_l1', 'lstm.weight_ih_l1_reverse', 'lstm.weight_hh_l1_reverse', 'lstm.bias_ih_l1_reverse', 'lstm.bias_hh_l1_reverse', 'classifier.weight', 'classifier.bias']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    }
   ],
   "source": [
    "type_model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", config=type_config)\n",
    "emotion_model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", config=emotion_config)\n",
    "time_model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", config=time_config)\n",
    "confidence_model = RobertaForSequenceClassification.from_pretrained(\"klue/roberta-base\", config=confidence_config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "42e629c9",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_model.to(gpu)\n",
    "\n",
    "batch_size = 64\n",
    "save_steps = 45"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "14a7c9e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./output_type\",\n",
    "    seed=seed,\n",
    "    save_total_limit=2,\n",
    "    save_steps = save_steps,\n",
    "    num_train_epochs = 3,\n",
    "    learning_rate= 1e-4,\n",
    "    per_device_train_batch_size=batch_size,\n",
    "    per_device_eval_batch_size=batch_size,\n",
    "    gradient_accumulation_steps=4,\n",
    "    weight_decay=1e-4,\n",
    "    logging_dir=\"./logs\",\n",
    "    logging_steps = save_steps,\n",
    "    evaluation_strategy = \"steps\",\n",
    "    metric_for_best_model = \"eval_f1\",\n",
    "    eval_steps = save_steps,\n",
    "    load_best_model_at_end=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3dfcbe98",
   "metadata": {},
   "outputs": [],
   "source": [
    "type_trainer = Trainer(\n",
    "    model=type_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_type_dataset,\n",
    "    eval_dataset=valid_type_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "type_trainer.train()\n",
    "type_model.save_pretrained(\"./model_type\")\n",
    "pred_tensor = type_trainer.predict(test_dataset)\n",
    "pred_type = pred_tensor.predictions.argmax(axis=1).tolist()\n",
    "\n",
    "emotion_trainer = Trainer(\n",
    "    model=emotion_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_emotion_dataset,\n",
    "    eval_dataset=valid_emotion_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "emotion_trainer.train()\n",
    "emotion_model.save_pretrained(\"./model_emotion\")\n",
    "pred_tensor = emotion_trainer.predict(test_dataset)\n",
    "pred_emotion = pred_tensor.predictions.argmax(axis=1).tolist()\n",
    "\n",
    "time_trainer = Trainer(\n",
    "    model=time_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_type_dataset,\n",
    "    eval_dataset=valid_type_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "time_trainer.train()\n",
    "\n",
    "time_model.save_pretrained(\"./model_time\")\n",
    "pred_tensor = time_trainer.predict(test_dataset)\n",
    "pred_time = pred_tensor.predictions.argmax(axis=1).tolist()\n",
    "\n",
    "confidence_trainer = Trainer(\n",
    "    model=confidence_model,\n",
    "    args=training_args,\n",
    "    train_dataset=train_type_dataset,\n",
    "    eval_dataset=valid_type_dataset,\n",
    "    compute_metrics=compute_metrics,\n",
    ")\n",
    "\n",
    "confidence_trainer.train()\n",
    "\n",
    "confidence_model.save_pretrained(\"./model_confidence\")\n",
    "pred_tensor = confidence_trainer.predict(test_dataset)\n",
    "pred_confidence = pred_tensor.predictions.argmax(axis=1).tolist()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
